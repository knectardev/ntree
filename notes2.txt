Below is a recommended set of defaults that fit your current direction (intraday, â€œfeature richness + minimal overlays,â€ RL-friendly, honest gating). Treat this as a strong starting spec you can tweak.

A) Data + targets

Base series

Use log price internally for modeling whenever possible.

If you want a single â€œprice-likeâ€ series for trend visuals, use OHLC4 for stability (less wick noise), but itâ€™s optional.

For everything predictive, use returns.

Returns definition

Default: log returns

ð‘Ÿ
ð‘¡
=
log
â¡
(
ð‘ƒ
ð‘¡
)
âˆ’
log
â¡
(
ð‘ƒ
ð‘¡
âˆ’
1
)
r
t
	â€‹

=log(P
t
	â€‹

)âˆ’log(P
tâˆ’1
	â€‹

)

Why: additive across time, less scale weirdness, works well with linear/state-space.

Forecast horizon(s)

Implement k âˆˆ {1, 3, 10} bars (these cover â€œnext bar,â€ â€œcouple bars,â€ â€œshort swing intradayâ€).

Compute features once, then produce target variants for RL/experiments.

Classification label y
Pick one â€œcanonicalâ€ and then allow variants:

Binary directional label (canonical)

y_up(k) = 1 if sum_{i=1..k} r_{t+i} > +thr(k) else 0

threshold: thr(k) = 0 (pure direction) to start

later add thr(k) = c * sigma_k (vol-adjusted) to avoid tiny-noise labels

De-overlap

For offline evaluation / classifier training: yes, use stride = k to reduce leakage.

For RL step-by-step environment: no, keep per-bar labels; RL will see overlap anyway.

B) Warmup + quality flags

is_warm

OLS: t >= window + 5

AR(p): t >= max(p) + window + 5

Kalman: t >= 20 (or 2Ã— your typical â€œsmoothing strengthâ€ in bars)

Classifier: needs a training buffer: t >= train_window

fit_ok defaults
Donâ€™t overthink. Provide both:

raw quality metrics (continuous)

a conservative boolean (for debugging + optional gating)

Recommended booleans:

OLS: RÂ² >= 0.05 AND |t_stat(mu_hat)| >= 1.0

AR: stability OK (roots) AND sigma_hat finite AND not exploding

Kalman: slope_z = slope / slope_std finite; optionally |slope_z| >= 0.5

Classifier: entropy <= 0.65 (or just output entropy; RL can learn)

But for RL: expose the booleans; donâ€™t hard-gate the feature stream. Let the agent learn to ignore bad regimes.

C) 1.1 Rolling OLS

Whatâ€™s X (regressors)?
Two variants (both useful):

Drift-only OLS (simple, robust)

Predict r_{t+1..t+k} from a small factor set:

trend_slope (from your local linear or Kalman slope)

vwap_dev (price - VWAP, z-scored)

vol_state (rolling std)

residual_energy (from detrend residual)

This is â€œARX-liteâ€ but easy.

Local linear trend OLS (time index)

Fit price ~ time over window â†’ outputs slope/RÂ²

You already basically have this.

Window + weighting

Start with fixed window OLS (simpler, comparable).

Add EWLS later if you want responsiveness.

Standard errors

Start with plain OLS SE.

Add Neweyâ€“West later only if youâ€™re doing statistical testing; for RL itâ€™s usually not necessary.

D) 1.2 AR / ARX

Orders

Start fixed: AR(1), AR(2), AR(3) features computed in parallel.

Avoid AIC/BIC auto-selection per bar at first (it adds jitter and complexity).

Exogenous terms
Keep a guaranteed small core set:

trend_slope_z (from Kalman or local linear)

vwap_dev_z

vol_z

session_time (optional: minutes since open as sin/cos)

Stability check

Provide both:

ar.is_stable boolean

ar.stability_margin continuous (distance of largest root from unit circle)

E) 1.3 Kalman local linear trend

Model form

Classic local linear trend:

state = [level, slope]

observation = level + noise

Noise parameters (Q/R)

Start fixed defaults + a single â€œresponsivenessâ€ knob.

Then optionally adapt:

R from rolling return variance

Q scaled to let slope change moderately under higher vol

Output units

Keep the filter on log price level

Output:

slope_per_bar in return units

slope_z = slope / slope_std (this is gold)

F) 1.4 Classifier (logistic / LDA)

Training

Start with rolling refit every N bars (e.g., every 25 bars) on last train_window (e.g., 500â€“2000 bars).

True per-bar online SGD is doable but you donâ€™t need it first.

Regularization

L2 on by default, fairly strong.

This reduces â€œspaghetti probability swings.â€

Calibration

Rolling Brier score over last 200â€“500 labeled examples.

Output:

clf.brier

clf.cal_ok = brier < threshold (optional)

G) Feature registry + naming

Yes: do stable names like:

kalman.level, kalman.slope, kalman.slope_z

ols.mu_hat.k1, ols.tstat.k1, ols.r2

ar1.mu_hat.k1, ar3.innov_z, ar2.stability_margin

clf.p_up.k3, clf.entropy, clf.brier
And always:

*.is_warm, *.fit_ok

H) Normalization standard

For RL, consistency matters more than perfection.

Recommended default:

Use rolling std of returns sigma (e.g., 100-bar) as the global scaler.

Rules:

Any return-like value â†’ divide by sigma

Any price-like deviation (VWAP dev) â†’ convert to return units or z-score

Then optionally apply per-feature z-score (but donâ€™t do both at once initially)

I) Visualization constraints (avoid spaghetti)

Hard cap: 1 trend overlay visible at once (SMA OR local linear OR Kalman).

Everything else goes into:

mini-panels (drift, slope_z, p_up)

background shading (regime/confidence)

â€œDeveloper modeâ€ shows:

residuals

innovations

covariances (optional)

Answering Cursorâ€™s â€œ3 key questionsâ€ (recommended defaults)

Prediction horizons: k = 1, 3, 10 bars

Returns vs price: compute predictors on returns; keep Kalman on log price but output return-normalized slope (slope_per_bar, slope_z)

Where is RL trained?

Best path: RL training elsewhere (Python) and the app emits features.

If you want in-browser later, keep it modular so you can swap. 

what bar size you expect RL to operate on most (1m, 5m, 15m?
Make bar size a config, and train/evaluate on a small set

Pick three canonical bar sizes and treat them like different â€œmarketsâ€:

1m = microstructure + noise, good for execution/entries

5m = the sweet spot for intraday structure/regimes

15m = less noise, more stable regimes, fewer decisions

Then:

compute the same feature schema for each

train the same RL policy separately per bar size

compare out-of-sample P&L and stability

This is the least opinionated, most informative path.

What to implement now

Your feature registry emits the same keys regardless of bar size

Windows are expressed in minutes, then converted to bars.

Example:

trend_window_minutes = 120

on 1m â†’ 120 bars

on 5m â†’ 24 bars

on 15m â†’ 8 bars

This keeps your â€œsmoothing strengthâ€ meaning consistent.