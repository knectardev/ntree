OK ultimately I would like to take these files, bundle the price data in a similar way that the Alpaca data has bundled the data at 1 minute resolution, but to bundle this 1 second resolution data to 30 seconds time intervals, then import these 6 month spans of data into my local SQLite db based on the existing schema (perhaps plus an additional "30 second resolution" field if that's useful. Can all of this be done in Python from cursor or terminal in the context of the cursor environment more efficiently?

and Python in Cursor/terminal is exactly the right way to do this efficiently. The key is to stream-decompress the .zst, aggregate on the fly to 30-second bars, and bulk-insert into SQLite in transactions. No GUI extraction, no giant intermediate JSON files.

Here’s a practical, “works in the real world” plan.

Recommended pipeline

Stream read *.json.zst (no full decompression to disk)

Parse each JSON line (each line = 1 second bar)

Bucket timestamps into 30-second intervals (floor to :00 or :30)

Aggregate OHLCV per bucket (and per symbol)

Write to SQLite in batches (e.g., 5k–50k rows per transaction)

Add indices + (optional) a timeframe_sec column or separate table

This will be dramatically faster than anything Explorer-based.

SQLite schema choice (do this, don’t overthink it)
Best option (clean + flexible)

Create a separate table for 30s bars:

bars_30s (or ohlcv_30s)

Same columns as your 1m Alpaca table

Same indexes (symbol, ts)

This avoids changing existing code paths.