Updates to RL 

Here’s what I’d do next (in a tight, deterministic order) to turn this into a usable, documentable range-cycle scalp learner.

1) Add one more safety test: “no-lookahead sentinel”

Right now you’ve got warmup + reward semantics locked. The remaining class of catastrophic bugs is feature leakage.

Test idea (cheap + brutal):

Create a synthetic price series.

Create a copy where you modify only the future portion (e.g., add a huge spike after index i+50).

Assert that get_observation(i) is identical between the original and modified series for all i before the spike window.

This catches:

accidental centered windows

accidental “fit full series”

any helper that slices too far

You can run it on just 2–3 representative feature groups to start.

2) Add “baseline parity” test: z-fade in env == z-fade in exporter

You’ve implemented z-fade baseline under the same rules. Great — now lock it:

Run z-fade baseline in the env runner (step-by-step)

Run z-fade baseline in the exported dataset runner (vectorized/offline if you have it)

Assert same trades/counts/costs/reward sum (or within epsilon)

This prevents “two subtly different baselines,” which always happens otherwise.

3) Instrumentation you’ll want before training (so you don’t fly blind)

Add these counters/series to info:

position (already implied but include explicitly)

time_in_position

breakout_flag

zscore

sigma

reward_components: {pnl, cost, breakout_penalty}

Even if you don’t log every bar in production, you want the option for:

“Why did it trade here?”

“Is it just churning?”

“Is breakout penalty dominating?”

This makes your UI explanations way easier later.

4) Training scaffolding: keep it minimal, but correct
Start with PPO (not recurrent) for the first run

Even though we expect RecurrentPPO to win later, PPO gives you a fast sanity check.

env: 1 trading day episodes, RTH

action: Discrete(4)

cost: 0.00005–0.00010 per change

reward: as implemented

The goal of PPO run #1 is not profit. It’s:

does learning improve vs random?

does turnover drop?

does it learn “don’t hold through breakout”?

Then switch to RecurrentPPO

Only after PPO is stable, because recurrent adds debugging complexity.

5) Evaluation protocol (the “documentable” part)

Before you chase Sharpe, lock these eval outputs every run:

Net PnL after costs (test split)

Max drawdown

Turnover (position changes/day)

Hit rate / average win/loss (optional, but nice)

“Breakout exposure”: % of time in position while breakout_flag=1

Comparison vs z-fade baseline

This is exactly what you’ll show in ntree as “RL policy health.”

6) Feature roadmap for range-cycle scalps (what to add next)

You don’t need the full cycle scan yet. Add one “cycle-ish” group now:

Band-pass proxy group

bp = EMA(close,10) - EMA(close,60)

bp_z (rolling z)

bp_slope (1-bar diff)

autocorr_ret_15, autocorr_ret_30, autocorr_ret_60 (optional)

This improves the agent’s ability to detect “bouncy vs dead” regimes without a heavy FFT pipeline.

Since you already reserved cycle-scan placeholders, you can keep schema stable.

Quick sanity check: your status implies you’re ready to train

Given:

schema stability ✅

warmup policy ✅

no-flip semantics ✅

cost model ✅

tests ✅

…you are at the point where the next meaningful artifact is a training script + eval report that produces:

PPO learning curve (reward)

test-period metrics vs baseline



