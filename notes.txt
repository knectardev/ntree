Here’s a tight spec you can hand to Cursor to implement LLM-generated interpretation + callouts as a sidecar artifact (no pixel arrows, no CV, no fragility).

LLM Annotation Sidecar Requirements (Option A)
Goal

After generating a behavior timeline (baseline vs PPO), produce a human-readable interpretation summary grounded strictly in:

the exported per-step timeline dataset (*.timeline.parquet/csv)

the per-day summary metrics (*.summary.json)

config/meta (schema/env_cfg)

No image parsing required.

Outputs

For each date rendered, create:

.../spy_1m_rth_YYYY-MM-DD.annotation.md

(optional but nice) .../spy_1m_rth_YYYY-MM-DD.annotation.json (structured)

CLI Flags

Add to scripts/render_behavior_timeline.py:

--llm_annotate (bool)

--llm_provider (string; default can be whatever you’re using)

--llm_model (string; optional)

--llm_max_tokens (int; e.g. 400)

--llm_temperature (float; e.g. 0.2)

--llm_offline (bool)

If set (or if provider unavailable), write a deterministic “template-only” summary using metrics (no LLM).

Inputs to the LLM (the “digest”)

Before calling the LLM, build a compact JSON digest (and save it to disk for traceability):

.../spy_1m_rth_YYYY-MM-DD.digest.json

Digest fields

identifiers:

symbol, bar_size, session_mode, date, timezone

schema_id

model_id (ppo path or checkpoint id; “none” if baseline-only)

baseline metrics:

total_reward, pnl, cost, breakout_penalty

turnover_changes, time_in_market_frac, avg_hold_min, breakout_exposure_pct

action_counts (H, L, S, X)

ppo metrics (same keys; omit if not provided)

key “events” extracted deterministically from the timeline:

top_trade_clusters: list of windows with unusually high turnover (e.g. [start_ts, end_ts, num_changes])

breakout_exposure_windows: windows where position!=0 and breakout_flag==1 (top 3 by duration)

largest_drawdown_window: [start_ts, end_ts] (approx; can be based on cumulative reward curve)

largest_pnl_gain_window: [start_ts, end_ts]

notable_divergence_windows: windows where baseline is flat but PPO is in position (and vice versa), top 3 by duration

Keep it small: 1–2 pages of JSON max.

LLM Prompt Template

System: You are a trading analytics explainer. No ML jargon. No hype. Be specific.
User: Given this digest JSON, write an interpretation of baseline vs PPO for a beginner.
Constraints:

120–180 words max (configurable)

Bullet lists preferred

Do not invent any metrics not in digest

Include 3 timestamped callouts using the digest windows

End with “One lever to try next” (one sentence) based on metrics (churn vs breakout vs hold time)

Annotation Markdown Format (exact)
# Behavior Timeline Interpretation — {SYMBOL} {BAR} {SESSION} ({DATE})

## Summary
{2–4 bullets comparing PPO vs baseline in plain English}

## What the baseline did
{2–3 bullets}

## What the learned policy did differently
{2–3 bullets}

## Notable moments (callouts)
- {HH:MM–HH:MM ET}: {callout}
- {HH:MM–HH:MM ET}: {callout}
- {HH:MM–HH:MM ET}: {callout}

## One lever to try next
{single sentence, e.g. “reduce churn by …”}

Determinism / Traceability

Always save digest.json

Save the exact prompt and LLM response metadata (model, temp, tokens) into annotation.json

If --llm_offline, produce the same markdown using a deterministic rule-based template.

Acceptance Criteria

Running:

python scripts/render_behavior_timeline.py --date 2024-01-05 --ppo_model_path ... --llm_annotate

Produces:

.timeline.png

.timeline.parquet

.summary.json

.digest.json

.annotation.md (and optionally .annotation.json)

Annotation correctly references:

churn/turnover differences

breakout exposure differences

hold time differences

and includes 3 timestamped callouts that correspond to extracted windows

One extra suggestion (tiny but high value)

Add a --annotation_tone flag with options:

plain (default)

clinical

friendly

Because you’ll quickly develop a preference.
